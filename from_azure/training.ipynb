{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1701419409079
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aapolina\\Anaconda3\\envs\\swiss-androsace\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import CLIPModel, AutoModel, CLIPProcessor\n",
        "from preprocessing import fcgr, protein_to_dna\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "#from CLIP.clip import clip\n",
        "#from CLIP.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "from clip import clip\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class image_title_dataset(Dataset):\n",
        "    def __init__(self, processor, images, labels, max_len):\n",
        "        # Initialize image paths and corresponding texts\n",
        "        self.images = images\n",
        "        self.max_len = max_len\n",
        "        # Tokenize text using CLIP's tokenizer\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Preprocess image using CLIP's preprocessing function\n",
        "        labels_short = self.labels[idx:idx + 1].tolist()[:][:self.max_len]\n",
        "\n",
        "        tokens = clip.tokenize(self.labels[idx:idx + 1].tolist()[:50], context_length = 256,truncate=True)[:1, :self.max_len]\n",
        "\n",
        "        if tokens.size()[1] < self.max_len:\n",
        "            tokens = torch.cat([tokens,\n",
        "                       torch.zeros(size=(1, self.max_len - tokens.size()[1]))], dim=1).type(\n",
        "                torch.int)\n",
        "        #attmask = torch.zeros()\n",
        "        #pixel_values = torch.transforms\n",
        "\n",
        "        inputs = self.processor(text=labels_short, images=self.images[idx:idx+1], return_tensors=\"pt\", padding=True)\n",
        "        inputs['input_ids'] = tokens\n",
        "        if inputs['attention_mask'].size()[1] > self.max_len:\n",
        "            inputs['attention_mask'] = inputs['attention_mask'][:1, :self.max_len]\n",
        "        inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.zeros(size=(1, len(inputs['input_ids'][0]) - len(inputs['attention_mask'][0])))], dim=1).type(torch.int)\n",
        "\n",
        "        #inputs['input_ids'] = torch.cat([inputs['input_ids'], torch.Tensor([[0] * (self.max_len - len(inputs['input_ids'][0]))])], dim=1)\n",
        "        #inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.Tensor([[0] * (self.max_len - len(inputs['attention_mask'][0]))])], dim=1)\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        if p.requires_grad:\n",
        "            p.grad.data = p.grad.data.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_training(model,loader_test, criterion):\n",
        "    loss_all = []\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, inputs in enumerate(loader_test):\n",
        "            outputs = model(input_ids=torch.Tensor(inputs['input_ids']).type(torch.int),attention_mask=torch.Tensor(inputs['attention_mask']).squeeze(1), pixel_values=torch.Tensor(inputs['pixel_values']).squeeze(1))\n",
        "            \n",
        "            logits_i = outputs.logits_per_image\n",
        "            logits_t = outputs.logits_per_text\n",
        "            #probs = logits.softmax(dim=1)\n",
        "\n",
        "            labels = torch.arange(0, logits_i.shape[0])\n",
        "            loss_i = criterion(logits_i, labels)\n",
        "            loss_t = criterion(logits_t, labels)\n",
        "\n",
        "            loss = (loss_i + loss_t)/2\n",
        "            loss_all.append(loss.item())\n",
        "\n",
        "    return np.mean(loss_all)\n",
        "        \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, processor, images, labels, epochs=1):\n",
        "    #tokenizer = get_or_build_tokenizer(labels)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #for img in images:\n",
        "\n",
        "    #labeltokens = clip.tokenize(labels.tolist(), 128)\n",
        "\n",
        "    #labeltokens = np.array([np.array(tokenizer.encode(label).ids) for label in labels])\n",
        "\n",
        "    #inputs = processor(text=labeltokens, images=images, return_tensors='pt', padding=True)\n",
        "    #inputs = processor(text=torch.Tensor(labeltokens[:1]), images=torch.permute(images[:1], dims=(0,2,3,1)) )\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = Adam(params, lr=1e-5, weight_decay=0.0001)\n",
        "    \n",
        "    n=len(labels)\n",
        "    assert n == images.shape[0]\n",
        "    \n",
        "    images_train = images[:int(n*0.9)]\n",
        "    images_test = images[int(n*0.9):]\n",
        "    labels_train = labels[:int(n*0.9)]\n",
        "    labels_test = labels[int(n*0.9):]\n",
        "\n",
        "    \n",
        "    dataset_train = image_title_dataset(processor, images_train, labels_train, 77) #instead of 77\n",
        "    loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
        "\n",
        "    dataset_test = image_title_dataset(processor, images_test, labels_test, 77) #instead of 77\n",
        "    loader_test = DataLoader(dataset_test, batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = []\n",
        "        print(f\"Epoch {epoch+1}/{epochs}--------------------------------\")\n",
        "        for i, inputs in enumerate(loader_train):\n",
        "            \n",
        "        #inputs = processor(text=labels[:].tolist(), images=images[:], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=torch.Tensor(inputs['input_ids']).type(torch.int),\n",
        "                attention_mask=torch.Tensor(inputs['attention_mask']).squeeze(1), pixel_values=torch.Tensor(inputs['pixel_values']).squeeze(1))\n",
        "\n",
        "            logits_i = outputs.logits_per_image\n",
        "            logits_t = outputs.logits_per_text\n",
        "            #probs = logits.softmax(dim=1)\n",
        "\n",
        "            labels = torch.arange(0, logits_i.shape[0])\n",
        "            loss_i = criterion(logits_i, labels)\n",
        "            loss_t = criterion(logits_t, labels)\n",
        "\n",
        "            loss = (loss_i + loss_t)/2\n",
        "            loss.backward()\n",
        "\n",
        "            if device == 'cpu':\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                convert_models_to_fp32(model)\n",
        "                optimizer.step()\n",
        "                clip.model.convert_weights(model)\n",
        "\n",
        "            print(f\"{loss.item()}... batch {i+1}/{len(loader_train)}\")\n",
        "            loss_epoch.append(loss.item())\n",
        "    \n",
        "            if i%10==0:\n",
        "                loss_epoch_test = evaluate_training(model,loader_test,criterion)\n",
        "                print(f\"Current eval loss after batch {i+1}/{len(loader_train)} epoch={epoch} = {loss_epoch_test}\")\n",
        "\n",
        "        loss_epoch = np.mean(loss_epoch)\n",
        "        print(f\"Loss epoch {epoch} = {loss_epoch}\")\n",
        "        \n",
        "        loss_epoch_test = evaluate_training(model,loader_test,criterion)\n",
        "        print(f\"Final eval loss epoch={epoch} = {loss_epoch_test}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n    model : torch.nn.Module\\n        The CLIP model\\n\\n\\n    preprocess : Callable[[PIL.Image], torch.Tensor]\\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model, processor = clip.load(\"ViT-B/32\",device=device,jit=True) #Must set jit=False for training\n",
        "\"\"\"\n",
        "    model : torch.nn.Module\n",
        "        The CLIP model\n",
        "\n",
        "\n",
        "    preprocess : Callable[[PIL.Image], torch.Tensor]\n",
        "        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "nsamples = 10000\n",
        "labelcol = \"Protein names\"\n",
        "inputcol = \"Sequence\"\n",
        "labels = pd.read_csv(r\"labels.csv\")[labelcol][:nsamples]\n",
        "inputs = pd.read_csv(r\"sequences.csv\")[inputcol][:nsamples]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = np.array([fcgr(seq, k=7) for seq in inputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = np.array([img/np.sum(img) for img in images])\n",
        "images = torch.Tensor(images).unsqueeze(1).repeat(1, 3, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (79 > 77). Running this sequence through the model will result in indexing errors\n",
            "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1--------------------------------\n",
            "5.394437789916992... batch 1/71\n",
            "Current eval loss after batch 1/71 epoch=0 = 5.459358990192413\n",
            "5.814339637756348... batch 2/71\n",
            "5.080859661102295... batch 3/71\n",
            "5.6517744064331055... batch 4/71\n",
            "4.987646102905273... batch 5/71\n",
            "4.921845436096191... batch 6/71\n",
            "4.908542633056641... batch 7/71\n",
            "4.8676652908325195... batch 8/71\n",
            "4.863919734954834... batch 9/71\n",
            "4.863617897033691... batch 10/71\n",
            "4.8658037185668945... batch 11/71\n",
            "Current eval loss after batch 11/71 epoch=0 = 4.837612330913544\n",
            "4.871415138244629... batch 12/71\n",
            "4.861046314239502... batch 13/71\n",
            "4.862253189086914... batch 14/71\n",
            "4.856645584106445... batch 15/71\n",
            "4.857817649841309... batch 16/71\n",
            "4.856191635131836... batch 17/71\n",
            "4.855138301849365... batch 18/71\n",
            "4.856529235839844... batch 19/71\n",
            "4.855803489685059... batch 20/71\n",
            "4.855349540710449... batch 21/71\n",
            "Current eval loss after batch 21/71 epoch=0 = 4.828933000564575\n",
            "4.854987144470215... batch 22/71\n",
            "4.855568885803223... batch 23/71\n"
          ]
        }
      ],
      "source": [
        "train(model, processor, images, labels, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "model_path = r\"C:\\Users\\aapolina\\CODE\\sdsc_hackatchon_genAI\\CLIP-DNA\\from_azure\"\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "\n",
        "\n",
        "model_name = f\"model_clop_({timestamp}).ckpt\"\n",
        "torch.save(model.state_dict(), os.path.join(model_path, model_name) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x =torch.load(os.path.join(model_path, r\"model_clop_(2023_12_01-11_43_07).ckpt\"))\n",
        "x.keys()\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "swiss-androsace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
